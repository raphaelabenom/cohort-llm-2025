{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f226ac10-2a1e-4649-a771-f89087217130",
   "metadata": {},
   "source": [
    "# Install Required Libraries\n",
    "\n",
    "- The qdrant-client package. We'll be using the Python client, but Qdrant also offers official clients for JavaScript/TypeScript, Go, and Rust, so you can choose the best fit for your own projects.\n",
    "- The fastembed package - an optimized embedding (data vectorization) solution designed specifically for Qdrant. Make sure you install version >= 1.14.2 to use the local inference with Qdrant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669f6c15-e289-46c9-bb1e-b4d86656b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -q \"qdrant-client[fastembed]>=1.14.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d475c9-2a0f-40f0-a753-617ef0f0a0a3",
   "metadata": {},
   "source": [
    "# Step 1: Import Required Libraries & Connect to Qdrant\n",
    "\n",
    "The QdrantClient class allows us to establish a connection to the Qdrant service,<br>\n",
    "while the models module provides definitions for various configurations and parameters we‚Äôll use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67815540-ed15-4f27-bd89-6d1625379ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6d330b0-6dfc-4586-aad5-3b087905f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client\n",
    "client =QdrantClient(\"http://localhost:6333\") # connects to local Qdrant instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee05b59-45d0-4a41-9152-b0b8dc664c63",
   "metadata": {},
   "source": [
    "# Step 2: Study the Dataset\n",
    "\n",
    "To build a working vector search solution (and, more generally, to understand if/when/how it‚Äôs needed), it's good to study the dataset and figure out the nature and structure of the data we‚Äôre working with, for example:\n",
    "\n",
    "- modality ‚Äî is it text, images, videos, a combination?\n",
    "- specifics ‚Äî if it‚Äôs text: language used, how big are the text pieces, are there any special characters, etc.\n",
    "\n",
    "It will help us define:\n",
    "\n",
    "- the right data \"schema\" (what to vectorize, what to store as metadata, etc);\n",
    "- the right embedding model (the best fit based on the domain, precision & resource requirements).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1460038-a04a-495a-a2b1-9c3c8e28c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ac00cae-8434-4893-b375-ce9d6f240c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'course': 'data-engineering-zoomcamp', 'documents': [{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  ‚ÄúOffice Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon‚Äôt forget to register in DataTalks.Club's Slack and join the channel.\", 'section': 'General course-related questions', 'question': 'Course - When will the course start?'}, {'text': 'GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites', 'section': 'General course-related questions', 'question': 'Course - What are the prerequisites for this course?'}, {'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So \n"
     ]
    }
   ],
   "source": [
    "print(str(documents_raw)[:1_000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a33ff-9a94-4c93-a204-9a463e66d632",
   "metadata": {},
   "source": [
    "Data already seems cleaned and chunked (i.e., divided into small pieces that embedding models can easily digest), so what's left is to define:\n",
    "\n",
    "- which fields could be used for semantic search ;\n",
    "- which fields should be stored as metadata, e.g. useable for filtering conditions;\n",
    "\n",
    "We have a dataset with three course types:\n",
    "data-engineering-zoomcamp, machine-learning-zoomcamp, and mlops-zoomcamp.\n",
    "Each course includes a collection of question and text (answer) pairs, along with the section the question refers to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31e7d9-dee7-4255-b934-0ce210982afe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_Which Fields Could Be Used for Semantic Search_<br>\n",
    "if we‚Äôre building a Q&A retrieval-augmented generation (RAG) system,\n",
    "it makes sense to store the text field (answers) as embeddings, and use vector search to find the most relevant answer to a given question query.\n",
    "\n",
    "_Which Fields Should Be Stored as Metadata_<br>\n",
    "For example, we could store the course and section fields as metadata.\n",
    "This way, we can filter search results when asking questions related to a specific course or a specific section.\n",
    "\n",
    "# Step 3: Choosing the Embedding Model with FastEmbed\n",
    "\n",
    "The choice of an embedding model depends on many factors:\n",
    "\n",
    "- The task, data modality, and data specifics;\n",
    "- The trade-off between search precision and resource usage (larger embeddings require more storage and memory);\n",
    "- The cost of inference (especially if you're using a third-party provider);\n",
    "- etc<br>\n",
    "  The best way to select an embedding model is to test and benchmark different options on your own data.<br>\n",
    "  In this notebook, we‚Äôre going to use [FastEmbed](https://github.com/qdrant/fastembed) as our embedding provider.\n",
    "\n",
    "## FastEmbed for Textual Data\n",
    "\n",
    "Let‚Äôs select an embedding model to use for our course question answers, stored in text fields, from the options supported by FastEmbed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33c208fb-f9d6-40b6-9a85-bdbf56ec26de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'BAAI/bge-base-en',\n",
       "  'sources': {'hf': 'Qdrant/fast-bge-base-en',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.42,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-base-en-v1.5',\n",
       "  'sources': {'hf': 'qdrant/bge-base-en-v1.5-onnx-q',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en-v1.5.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.21,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-large-en-v1.5',\n",
       "  'sources': {'hf': 'qdrant/bge-large-en-v1.5-onnx',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 1.2,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-small-en',\n",
       "  'sources': {'hf': 'Qdrant/bge-small-en',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/BAAI-bge-small-en.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.13,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-small-en-v1.5',\n",
       "  'sources': {'hf': 'qdrant/bge-small-en-v1.5-onnx-q',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.067,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-small-zh-v1.5',\n",
       "  'sources': {'hf': 'Qdrant/bge-small-zh-v1.5',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Chinese, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.09,\n",
       "  'additional_files': [],\n",
       "  'dim': 512,\n",
       "  'tasks': {}},\n",
       " {'model': 'mixedbread-ai/mxbai-embed-large-v1',\n",
       "  'sources': {'hf': 'mixedbread-ai/mxbai-embed-large-v1',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-xs',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-xs',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.09,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-s',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-s',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.13,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-m',\n",
       "  'sources': {'hf': 'Snowflake/snowflake-arctic-embed-m',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.43,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-m-long',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-m-long',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 2048 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.54,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-l',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-l',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 1.02,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-clip-v1',\n",
       "  'sources': {'hf': 'jinaai/jina-clip-v1',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/text_model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text&image), English, Prefixes for queries/documents: not necessary, 2024 year',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.55,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'Qdrant/clip-ViT-B-32-text',\n",
       "  'sources': {'hf': 'Qdrant/clip-ViT-B-32-text',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text&image), English, 77 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.25,\n",
       "  'additional_files': [],\n",
       "  'dim': 512,\n",
       "  'tasks': {}},\n",
       " {'model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'sources': {'hf': 'qdrant/all-MiniLM-L6-v2-onnx',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 256 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.09,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-en',\n",
       "  'sources': {'hf': 'xenova/jina-embeddings-v2-base-en',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.52,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-small-en',\n",
       "  'sources': {'hf': 'xenova/jina-embeddings-v2-small-en',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.12,\n",
       "  'additional_files': [],\n",
       "  'dim': 512,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-de',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-de',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model_fp16.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (German, English), 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.32,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-code',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-code',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (English, 30 programming languages), 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-zh',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-zh',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), supports mixed Chinese-English input text, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-es',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-es',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), supports mixed Spanish-English input text, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'thenlper/gte-base',\n",
       "  'sources': {'hf': 'thenlper/gte-base',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'General text embeddings, Unimodal (text), supports English only input text, 512 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.44,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'thenlper/gte-large',\n",
       "  'sources': {'hf': 'qdrant/gte-large-onnx',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 1.2,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'nomic-ai/nomic-embed-text-v1.5',\n",
       "  'sources': {'hf': 'nomic-ai/nomic-embed-text-v1.5',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.52,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'nomic-ai/nomic-embed-text-v1.5-Q',\n",
       "  'sources': {'hf': 'nomic-ai/nomic-embed-text-v1.5',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model_quantized.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.13,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'nomic-ai/nomic-embed-text-v1',\n",
       "  'sources': {'hf': 'nomic-ai/nomic-embed-text-v1',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.52,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
       "  'sources': {'hf': 'qdrant/paraphrase-multilingual-MiniLM-L12-v2-onnx-Q',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (~50 languages), 512 input tokens truncation, Prefixes for queries/documents: not necessary, 2019 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.22,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
       "  'sources': {'hf': 'xenova/paraphrase-multilingual-mpnet-base-v2',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (~50 languages), 384 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 1.0,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'intfloat/multilingual-e5-large',\n",
       "  'sources': {'hf': 'qdrant/multilingual-e5-large-onnx',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-multilingual-e5-large.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (~100 languages), 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 2.24,\n",
       "  'additional_files': ['model.onnx_data'],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v3',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v3',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Multi-task unimodal (text) embedding model, multi-lingual (~100), 1024 tokens truncation, and 8192 sequence length. Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'cc-by-nc-4.0',\n",
       "  'size_in_GB': 2.29,\n",
       "  'additional_files': ['onnx/model.onnx_data'],\n",
       "  'dim': 1024,\n",
       "  'tasks': {'retrieval.query': 0,\n",
       "   'retrieval.passage': 1,\n",
       "   'separation': 2,\n",
       "   'classification': 3,\n",
       "   'text-matching': 4}}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastembed import TextEmbedding\n",
    "TextEmbedding.list_supported_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505d2e4-8107-406f-9dc6-8999f6ea91a8",
   "metadata": {},
   "source": [
    "Choose a model that produces small-to-moderate-sized embeddings (e.g., 512 dimensions), so we don‚Äôt overuse resources in our simple setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ce02f2a-4fa3-4391-937e-e53a5a75a0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"BAAI/bge-small-zh-v1.5\",\n",
      "  \"sources\": {\n",
      "    \"hf\": \"Qdrant/bge-small-zh-v1.5\",\n",
      "    \"url\": \"https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz\",\n",
      "    \"_deprecated_tar_struct\": true\n",
      "  },\n",
      "  \"model_file\": \"model_optimized.onnx\",\n",
      "  \"description\": \"Text embeddings, Unimodal (text), Chinese, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.\",\n",
      "  \"license\": \"mit\",\n",
      "  \"size_in_GB\": 0.09,\n",
      "  \"additional_files\": [],\n",
      "  \"dim\": 512,\n",
      "  \"tasks\": {}\n",
      "}\n",
      "{\n",
      "  \"model\": \"Qdrant/clip-ViT-B-32-text\",\n",
      "  \"sources\": {\n",
      "    \"hf\": \"Qdrant/clip-ViT-B-32-text\",\n",
      "    \"url\": null,\n",
      "    \"_deprecated_tar_struct\": false\n",
      "  },\n",
      "  \"model_file\": \"model.onnx\",\n",
      "  \"description\": \"Text embeddings, Multimodal (text&image), English, 77 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year\",\n",
      "  \"license\": \"mit\",\n",
      "  \"size_in_GB\": 0.25,\n",
      "  \"additional_files\": [],\n",
      "  \"dim\": 512,\n",
      "  \"tasks\": {}\n",
      "}\n",
      "{\n",
      "  \"model\": \"jinaai/jina-embeddings-v2-small-en\",\n",
      "  \"sources\": {\n",
      "    \"hf\": \"xenova/jina-embeddings-v2-small-en\",\n",
      "    \"url\": null,\n",
      "    \"_deprecated_tar_struct\": false\n",
      "  },\n",
      "  \"model_file\": \"onnx/model.onnx\",\n",
      "  \"description\": \"Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.\",\n",
      "  \"license\": \"apache-2.0\",\n",
      "  \"size_in_GB\": 0.12,\n",
      "  \"additional_files\": [],\n",
      "  \"dim\": 512,\n",
      "  \"tasks\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "EMBEDDING_DIMENSIONALITY = 512\n",
    "\n",
    "for model in TextEmbedding.list_supported_models():\n",
    "    if model['dim'] == EMBEDDING_DIMENSIONALITY:\n",
    "        print(json.dumps(model, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ceaf2a-8529-499d-8b91-ef61720bdede",
   "metadata": {},
   "source": [
    "We need an embedding model suitable for English text.\n",
    "<br><br>\n",
    "It also makes sense to select a unimodal model, since we‚Äôre not including images in our search, and specifically tailored solutions are usually better than universal ones.\n",
    "<br><br>\n",
    "It seems like **jina-embedding-small-en** is a good choice!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bec165d-b6dc-4d8f-9b66-497001f8aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df104e4-9ec5-4f82-b18b-71c3aec06476",
   "metadata": {},
   "source": [
    "jina-embedding-small-en was trained to measure semantic closeness using cosine similarity. You can find this information, for example, on the model‚Äôs [Hugging Face card](https://huggingface.co/jinaai/jina-embeddings-v2-small-en).<br>\n",
    "\n",
    "The parameters of the chosen embedding model, including the output embedding dimensions and the semantic similarity (distance) metric, are required to configure semantic search in Qdrant.<br>\n",
    "\n",
    "Here‚Äôs a quick overview of Qdrant‚Äôs core terminology:\n",
    "\n",
    "- **Points** are the central entity Qdrant works with.\n",
    "  <br>A point is a record consisting of an **ID**, a **vector**, and an optional **payload**.\n",
    "- A **collection** is a named set of points (i.e., vectors with optional payloads) that you can search within.\n",
    "  <br>Think of it as the container for your vector search solution, a single business problem solved.\n",
    "\n",
    "Qdrant supports different types of vectors to enable different modes of data exploration and search (dense, sparse, multivectors, and named vectors).\n",
    "\n",
    "In this example, we‚Äôll use the most common type, **dense vectors**.\n",
    "\n",
    "Embeddings capture the semantic essence of the data, while the **payload** holds structured metadata.<br>\n",
    "This metadata becomes especially useful when applying filters or sorting during search. Qdrant's payloads can hold structured data like booleans, keywords, geo-locations, arrays, and nested objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a04127c-eb7a-493a-b24b-8313353fc352",
   "metadata": {},
   "source": [
    "# Step 4: Create a Collection\n",
    "\n",
    "When creating a [collection](https://qdrant.tech/documentation/concepts/collections/), we need to specify:\n",
    "\n",
    "- Name: A unique identifier for the collection.\n",
    "- Vector Configuration:\n",
    "  - Size: The dimensionality of the vectors.\n",
    "  - Distance Metric: The method used to measure similarity between vectors.\n",
    "\n",
    "There are additional parameters you can explore in our [documentation](https://qdrant.tech/documentation/concepts/collections/#create-a-collection). Moreover, you can configure other vector types in Qdrant beyond typical dense embeddings (f.e., for hybrid search). However, for this example, the simplest default configuration is sufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a698771e-e228-43f2-8d0a-cc8b78fecede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the collection name\n",
    "collection_name = 'zoomcamp-rag'\n",
    "\n",
    "# Create the collection with the specified vector parameters\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=EMBEDDING_DIMENSIONALITY,  # Dimensionality of the vectors\n",
    "        distance=models.Distance.COSINE  # Distance metric for similarity search\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b161e59-3860-431b-85f5-91f2b00149c1",
   "metadata": {},
   "source": [
    "# Step 5: Create, Embed & Insert Points into the Collection\n",
    "\n",
    "[Points](https://qdrant.tech/documentation/concepts/points/#points) are the core data entities in Qdrant. Each point consists of:\n",
    "\n",
    "1. **ID**. A unique identifier. Qdrant supports both 64-bit unsigned integers and UUIDs.\n",
    "2. **Vector**. The embedding that represents the data point in vector space.\n",
    "3. **Payload** (optional). Additional metadata as key-value pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c021461-8fb2-46dc-b850-28c587e8254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "id = 0\n",
    "\n",
    "for course in documents_raw:\n",
    "    for doc in course['documents']:\n",
    "\n",
    "        point = models.PointStruct(\n",
    "            id=id,\n",
    "            vector=models.Document(text=doc['text'], model=model_handle), #embed text locally with \"jinaai/jina-embeddings-v2-small-en\" from FastEmbed\n",
    "            payload={\n",
    "                'text': doc['text'],\n",
    "                'section': doc['section'],\n",
    "                'course': course['course']\n",
    "            } #save all needed metadata fields\n",
    "        )\n",
    "        points.append(point)\n",
    "\n",
    "        id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ae493d9-b714-4f75-a7bf-c0cf742e386c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('id', 0)\n",
      "('vector', Document(text=\"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  ‚ÄúOffice Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon‚Äôt forget to register in DataTalks.Club's Slack and join the channel.\", model='jinaai/jina-embeddings-v2-small-en', options=None))\n",
      "('payload', {'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  ‚ÄúOffice Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon‚Äôt forget to register in DataTalks.Club's Slack and join the channel.\", 'section': 'General course-related questions', 'course': 'data-engineering-zoomcamp'})\n"
     ]
    }
   ],
   "source": [
    "for data in points[0]:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee847151-d57d-4ccf-b908-5f1fe4ff02a0",
   "metadata": {},
   "source": [
    "Now we‚Äôre going to embed and upload points to our collection.\n",
    "\n",
    "First, FastEmbed will fetch&download the selected model (path defaults to os.path.join(tempfile.gettempdir(), \"fastembed_cache\")), and perform inference directly on your machine.\n",
    "Then, the generated points will be upserted into the collection, and the vector index will be built.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def0447-66f6-40d6-a07d-d5e84decedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24e85d-4d60-4eac-935d-99537bdcc032",
   "metadata": {},
   "source": [
    "The speed of upsert mainly depends on the time spent on local inference.<br>\n",
    "To speed this up, you could run FastEmbed on GPUs or use a machine with more resources.\n",
    "\n",
    "In addition to basic upsert, Qdrant supports **batch upsert** in both column- and record-oriented formats.\n",
    "\n",
    "The Python client offers:\n",
    "\n",
    "- Parallelization\n",
    "- Retries\n",
    "- Lazy batching\n",
    "\n",
    "These can be configured via parameters in the upload_collection and upload_points functions.\n",
    "For details, check the [documentation](https://qdrant.tech/documentation/concepts/points/#upload-points).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16775330-fa41-4b63-a5f4-7277933a4454",
   "metadata": {},
   "source": [
    "## Study Data Visually\n",
    "\n",
    "Let‚Äôs explore the uploaded data in the Qdrant Web UI at http://localhost:6333/dashboard to study semantic similarity visually.\n",
    "\n",
    "For example, using the Visualize tab in the zoomcamp-rag collection, we can view all answers to the course questions (948 points) and see how they group together by meaning, additionally coloured by the course type.\n",
    "\n",
    "To do that, run the following command:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"limit\": 948,\n",
    "  \"color_by\": {\n",
    "    \"payload\": \"course\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This 2D representation is the result of dimensionality reduction applied to jina-embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff90bbe7-8e41-42b8-b681-8d452e4592d1",
   "metadata": {},
   "source": [
    "# Step 6: Running a Similarity Search\n",
    "\n",
    "Now, let‚Äôs find the most similar text vector in Qdrant to a given query embedding - the most relevant answer to a given question.\n",
    "How Similarity Search Works\n",
    "\n",
    "1. Qdrant compares the query vector to stored vectors (based on a vector index) using the distance metric defined when creating the collection.\n",
    "2. The closest matches are returned, ranked by similarity.\n",
    "   Vector index is built for **approximate** nearest neighbor (ANN) search, making large-scale vector search feasible.\n",
    "\n",
    "If you'd like to dive into our choice of vector index for vector search, check our article [\"What is a vector database\"](https://qdrant.tech/articles/what-is-a-vector-database/), or, for a more technical deep dive, our article on [Filterable Hierarchical Navigable Small World](https://qdrant.tech/articles/filtrable-hnsw/).\n",
    "\n",
    "Let's define a search function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60e97e0a-b78a-49b0-90c3-59a8abfb39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, limit=1):\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document( #embed the query text locally with \"jinaai/jina-embeddings-v2-small-en\"\n",
    "            text=query,\n",
    "            model=model_handle \n",
    "        ),\n",
    "        limit=limit, # top closest matches\n",
    "        with_payload=True #to get metadata in the results\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1789ae-b7ef-46f9-a8fc-e548364abdc3",
   "metadata": {},
   "source": [
    "Now let‚Äôs pick a random question from the course data.<br>\n",
    "As you remember, we didn‚Äôt upload the questions to Qdrant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd685072-e424-443e-bde5-ffc1f95f5f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"When creating a duplicate of your dataframe by doing the following:\\nX_train = df_train\\nX_val = df_val\\nYou\\u2019re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\\nX_train = df_train.copy()\\nX_val = df_val.copy()\\nAdded by Ixchel Garc\\u00eda\",\n",
      "  \"section\": \"2. Machine Learning for Regression\",\n",
      "  \"question\": \"Null column is appearing even if I applied .fillna()\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "course = random.choice(documents_raw)\n",
    "course_piece = random.choice(course['documents'])\n",
    "print(json.dumps(course_piece, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d85fe9-64e4-41a8-b9b0-780cf084fac9",
   "metadata": {},
   "source": [
    "Let's see which answer we get:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e5a9826-ed9d-4138-9c1f-1496cd238f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = search(course_piece['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7d0bdd5-8626-4054-b7bb-0c23e83dc082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryResponse(points=[ScoredPoint(id=286, version=0, score=0.79871285, payload={'text': 'If you encounter data type error on trip_type column, it may due to some nan values that isn‚Äôt null in bigquery.\\nSolution: try casting it to FLOAT datatype instead of NUMERIC', 'section': 'Module 4: analytics engineering with dbt', 'course': 'data-engineering-zoomcamp'}, vector=None, shard_key=None, order_value=None)])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029cb0e-e02c-400b-ac61-1ff61d1dc19d",
   "metadata": {},
   "source": [
    "Let‚Äôs compare the original and retrieved answers for our randomly selected question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c2b52a4-2285-4255-acc4-1bb7788521a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Null column is appearing even if I applied .fillna()\n",
      "\n",
      "Top Retrieved Answer:\n",
      "If you encounter data type error on trip_type column, it may due to some nan values that isn‚Äôt null in bigquery.\n",
      "Solution: try casting it to FLOAT datatype instead of NUMERIC\n",
      "\n",
      "Original Answer:\n",
      "When creating a duplicate of your dataframe by doing the following:\n",
      "X_train = df_train\n",
      "X_val = df_val\n",
      "You‚Äôre still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\n",
      "X_train = df_train.copy()\n",
      "X_val = df_val.copy()\n",
      "Added by Ixchel Garc√≠a\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question:\\n{course_piece['question']}\\n\")\n",
    "print(\"Top Retrieved Answer:\\n{}\\n\".format(result.points[0].payload['text']))\n",
    "print(\"Original Answer:\\n{}\".format(course_piece['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e4a992-5a08-48b6-86dc-f63adb6ce588",
   "metadata": {},
   "source": [
    "Now let‚Äôs search the answer to a question that wasn‚Äôt in the initial dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "16673b38-f3d4-4a83-b66b-0070048a4e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, late submissions are not allowed. But if the form is still not closed and it‚Äôs after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\n",
      "Older news:[source1] [source2]\n"
     ]
    }
   ],
   "source": [
    "print(search(\"What if I submit homeworks late?\").points[0].payload['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9438e-ae38-41c5-ba8a-2da4b833cf53",
   "metadata": {},
   "source": [
    "# Step 7: Running a Similarity Search with Filters\n",
    "\n",
    "We can refine our search using metadata filters.\n",
    "\n",
    "- Qdrant‚Äôs custom vector index implementation, Filterable HNSW, allows for precise and scalable vector search with filtering conditions.\n",
    "\n",
    "For example, we can search for an answer to a question related to a specific course from the three available in the dataset.\n",
    "Using a must filter ensures that all specified conditions are met for a data point to be included in the search results.\n",
    "\n",
    "- Qdrant also supports other filter types such as should, must_not, range, and more. For a full overview, check our [Filtering Guide](https://qdrant.tech/articles/vector-search-filtering/)\n",
    "\n",
    "To enable efficient filtering, we need to turn on [indexing of payload fields.](https://qdrant.tech/documentation/concepts/indexing/#payload-index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5e9071f-5da1-4d3a-8419-20d88aff05b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=2, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"course\",\n",
    "    field_schema=\"keyword\" # exact matching on string metadata fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd6389-dd8f-4ccf-a1a8-a1a805f346db",
   "metadata": {},
   "source": [
    "Now let's update our search function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33710b98-8961-4e99-aa6d-a7b0df311afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_course(query, course=\"mlops-zoomcamp\", limit=1):\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document( #embed the query text locally with \"jinaai/jina-embeddings-v2-small-en\"\n",
    "            text=query,\n",
    "            model=model_handle\n",
    "        ),\n",
    "        query_filter=models.Filter( # filter by course name\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"course\",\n",
    "                    match=models.MatchValue(value=course)\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        limit=limit, # top closest matches\n",
    "        with_payload=True #to get metadata in the results\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce33257-c672-449c-acf5-b3bbdbe4624d",
   "metadata": {},
   "source": [
    "Let‚Äôs see how the same question is answered across different courses:<br>\n",
    "data-engineering-zoomcamp, machine-learning-zoomcamp, and mlops-zoomcamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc0a5e2e-3686-4080-a0e2-30a7c004441b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose the closest one to your answer. Also do not post your answer in the course slack channel.\n"
     ]
    }
   ],
   "source": [
    "print(search_in_course(\"What if I submit homeworks late?\", \"mlops-zoomcamp\").points[0].payload['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625e60c-b849-459a-ad7a-2e60fb78baad",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "üéâ Congratulations! I now have everything I need to run a simple semantic search with Qdrant! üëè\n",
    "\n",
    "In general, data preparation, organization, and storage in a production-ready vector search solution is a topic worth a course of its own.\n",
    "If you‚Äôre curious to dive deeper into efficient vector search setup, check out our [Vector Search Manuals](https://qdrant.tech/articles/vector-search-manuals/).\n",
    "\n",
    "In the next videos, we will show you how to use [hybrid search](https://qdrant.tech/articles/hybrid-search/), combining the strengths of both keywords-based search and vector search. In many real-world applications, they work hand-in-hand, balancing the precision of keywords with the flexibility of embeddings to deliver the best results.\n",
    "\n",
    "P.S. We encourage you to check out Qdrant‚Äôs capabilities, which go beyond similarity search powering RAG & agentic pipelines (but still, here's our [MCP server](https://github.com/qdrant/mcp-server-qdrant) ;) ).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
